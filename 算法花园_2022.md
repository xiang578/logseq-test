---
public: true
title: 算法花园/2022
tags:
date: 2024-10-05
updated: 2024-10-05
toc: true
mathjax: true
---

## [[2022/12]]

  + [[@翦商：殷周之变与华夏新生]] 历史小说，讲得是夏商周的历史。上古史的参考文献很少，作者亮点在于结合这几十年的考古成果（二里头、殷墟、周原等等）进行创造。翦商出自诗经「后稷之孙，实维太王，居岐之阳，实始翦商。」，意思是周灭商。为什么周要消灭商？商代遗址挖掘发现和甲骨文破译，商人生活中使用大量的人牲进行祭祀（比如房屋用人奠基，墓葬用人陪葬……），而且随着时间推移，祭祀规模越来越大，手段越来越残酷。作者推测周族可能长时间从事给商人捕捉祭祀用的人牲工作，随着周文王被拘和伯邑考被杀，周商矛盾激化，周武王开始伐商。周公旦在武王驾崩后，辅佐成王，创建礼乐制度。后世流传的文献没有直接记录商朝活人祭祀，可能是周公毁掉相关的档案，改写历史。

## [[2022/11]]

  + [[简悦和 Logseq 联动]]

  + 当前使用[[词典]]


    + Oxford Essential Dictionary

    + Collins Cobuild Essential English Dictionary

    + Collins Cobuild Intermediate Learner’s Dictionary

    + OALD4双解切换2022秋季版

    + Oxford Advanced Learner's Dictionary 10th

  + [[Word2Vec]]

  + [[SGD]]

    + [[@Adam: A Method for Stochastic Optimization]]

  + [[@Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting]] 清华 [[NeurIPS/2021]]

  + [[@Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting]]

  + [[@一点微小的 OneNote 使用经验]]

## [[2022/10]]

  + 日常摘录 [[2022week41]]、[[2022week42]]

  + [[Positional Encoding]]

    + [[Positional Encoding/Sinusoidal]]

  + [[@Transformers in Time Series: A Survey]] 阿里达摩院关于 [[Transformer]] 在[[时间序列预测]]应用的综述

    + [[@Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting]] 北航 AAAI 2021 best paper，长时间序列预测任务引入 transformer。根据 attention score 的长尾分布，设计 probsparse self-attention 减少 query 和 key 计算量。然后使用 self-attention distilling 减少多层 attention 结构的内存开销。decoder 模块改成 generative style 预测方法，加快预测速度。

  + [[@FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction]]

  + [[@中国未来20年的经济大趋势 旧]]

  + [[@天空之城：拉马努金式思维训练法]]

  + [[how to read a book]]

    + [[@到底要如何有效读书呢 - 2021年读书计划， 最多只读50本]]

    + [[@Reading Better: Retaining and Applying What You Read]] #[[Farnam Street]]

    + [[@读书是否是唯一重要的事？]]

    + [[@如何阅读一本书]]

  + [[how to read a paper]]

    + [[知识管理带动文献阅读]]

    + [[戴晓天论文笔记模板]]

  + [[@Distilling the Knowledge in a Neural Network]]

  + [[@Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction]]

  + [[@Applying Deep Learning To Airbnb Search]]

## [[2022/09]]

  + 日常摘录 [[2022week40]]

  + [[Python/Decorator]] 装饰器

  + [[Stop Taking Regular Notes Use a Zettelkasten Instead]]，[[Eugene Yan]] 介绍如何用 Zettelkasten 笔记法学习机器学习知识

  + [[@Order Fulfillment Cycle Time Estimation for On-Demand Food Delivery]] 饿了么配送 ETA

    + [[Box-Cox Transformation]]

    + [[Log-Normal Distribution]]

  + [[Soren Bjornstad]]

    + [[Effective Flashcard Writing: Decomposing half a chapter of Thinking, Fast and Slow]] 以读[[思考快与慢]]为例，介绍如何制作记忆卡片

    + [[@A Tour Through My Zettelkasten]] 介绍如何使用 tiddlywiki 构建一个卡片管理系统

    + [[Spaced Repetition Forget about forgetting]] 介绍间隔重复背后的原理、 Anki 和 Remnote 的实践经验

  + [[杀戮尖塔]]

  + [[时序特征]]

  + [[FM]] [[AFM]] [[FFM]] [[@FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction]] [[NFM]]

    + 优化算法 [[FTRL]]

  + [[GBDT]] [[XGBoost]] [[LightGBM]]

    + [[DART]] 基于 dropout 思想的提升树方法

  + [[Transformer]]

    + [[Attention]] [[self-attention]]

## [[2022/08]]

  + [[序数回归]] 考虑类别间关系的分类算法

  + [[@Applying Deep Learning To Airbnb Search]] Airbnb 搜索推荐从 GBDT 迁移到深度学习的尝试记录

  + [[STGCN]] 时空图卷积网络：用于交通预测的深度学习框架

  + [[从加减乘除到机器学习]] 机器学习相关的数学书


    + [[从加减乘除到机器学习/数学要素]]

    + [[从加减乘除到机器学习/矩阵力量]]

      + [[矩阵]]

  + [[福格行为模型]]

  + [[how to read a book]] 收集的读书方法汇总

## [[2022/07]]

  + [[Life Long Learning]] 根据李宏毅课程整理的增量学习相关笔记

  + [[Multi-Task Learning]]

  + [[@置身事内：中国政府与经济发展]]

  + [[杀戮尖塔]]

  + [[GNN]]

## [[2022/01]]

  + [[沸腾新十年]]

  + [[Momentum Contrast for Unsupervised Visual Representation Learning]]
  + [[MacOS]]

  + [[Logseq]]

  + [[竞赛性编程]]
