---
title: "集成学习"
alias:
  - "ensemble learning"
public: true
tags:
date: "2024-10-05"
updated: "2025-03-27"
toc: true
mathjax: true
---

分类

  + Boosting 减少偏差
    + 串行训练

    + 逐步聚焦基分类器分错的样本，减少集成分类器的偏差。

  + Bagging 减少方差
    + 并行训练

    + 多次对训练样本进行采样，并分别训练多个不同的模型，然后做综合，减少集成分类器的方差。
  + [[stacking]] 提升预测精度

    + 一个母模型整合多个子模型

    + 子模型用整个训练集来训练，但是使用的算法不同，异质集成

基本步骤

  + 找到误差相互独立的基分类器

  + 训练基分类器

  + 合并基分类器的结果

    + voting

    + [[stacking]]

      + [[Adaboost]] 对数据处理而非算法进行处理，使得下一次子模型使用的训练数据更加侧重上次预测不准的部分。

        + 对分类正确的样本降低权重

        + 对错误分类的样本升高或者保持权重不变

        + 在模型融合过程中，根据错误率对基分类器进行加权融合，错误率低的分类器拥有更大话语权

基分类器

  + 可否将随机森林中的基分类器，由决策树替换为线性分类器或K-近邻？请解释为什么？

    + Bagging 好处集成后的分类器方差比基分类器小

    + 基分类器最好是不稳定的分类器，对样本分布比较敏感。

    + 线性分类器或K-近邻都是比较稳定的分类器，bagging 并不能在原有的分类器基础上获得更好的表现。

      + bagging 中对数据进行采样，可能导致他们比较难收敛，增大偏差。

  + 可以将基分类器换成神经网络

[[Q - 集成学习是否能学到子分类器没有的新东西？]]



[[Ref]]

  + [使用sklearn进行集成学习——实践 - jasonfreak - 博客园 (cnblogs.com)](https://www.cnblogs.com/jasonfreak/p/5720137.html)
