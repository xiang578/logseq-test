---
tags:
  - web
  - ReRank
public: true
categories: 随手记
toc: true
date:
  - 2024/06/01
permalink: note/wx_OTyEbPCBh1NHogPM7bBtvA
title: @重排序在快手短视频推荐系统中的演进
updated: 2024-10-05
mathjax: true
---

链接：[渠江涛：重排序在快手短视频推荐系统中的演进 (qq.com)](https://mp.weixin.qq.com/s/OTyEbPCBh1NHogPM7bBtvA)

<!--more-->

## 想法

  + 值得一看，混排强化学习那部分设计还不是很熟悉

  + 端上重排，如何拆分模型部署在服务器和端上

## 摘录

  + ### 快手推荐环节

![image.png](/assets/image_1717230662765_0.png)

  + ### 序列重排

    + 重排需要解决的问题

      + 整个序列的价值并非单 item 效果的累计，如何使得序列价值最大化；

      + 沉浸式场景中，什么是好的多样性体验，业务意志如何体现；

      + 同一个场景下越来越多的业务参与其中，如何恰当地分配流量和注意力，达成业务目标和整体最优；

      + 如何更加及时、更加细微地感知用户状态，及时调整我们的推荐策略和内容。

    + generator-evaluator 范式

      + generator 从 top50 中生成模式丰富的序列类型

      + 然后使用 evaluator 评价召回的序列整体价值

    + #### 序列生成

      + 通过协同采样生成多序列，采样过程从原理上来讲是不断地逼近 Pareto 最优曲面，进而得到不同的采样点，形成不同的序列。

![image.png](/assets/image_1717230143544_0.png)

    + #### [[Rerank/Evaluator]]

      + 三个业务目标

      + 单向 Transformer (用户自上而下刷视频，下游视频对上游视频没有增益)，也可以降低复杂度，提升模型稳定性。

![image.png](/assets/image_1717230456528_0.png)

      + 

  + ### 序列混排

    + #### Base 方案

      + 混排问题定义：将各个业务返回结果恰当地组合，得到社会综合价值最大的返回序列。

      + LinkedIn 优化目标

        + 在用户价值体验大于C的前提下最优化营收价值

![image.png](/assets/image_1717230959521_0.png)

      + base 方案的问题

![image.png](/assets/image_1717231164041_0.png)

    + #### 混排 listwise 方案

      + 跨域转化模块，广告和自然内容是跨域的

      + 广告内容多任务预估，利用左侧短视频信息和 context 信息校准广告 ctr 和 cvr 等指标

![image.png](/assets/image_1717231250657_0.png)

    + #### 混排 RL 方案

      + 目标：长期体验和近期收入平衡

      + 状态、动作、回报

![image.png](/assets/image_1717231516155_0.png)

      + [[Dueling DQN]]

        + 首先，V网络评估用户当前的满意程度，这使得模型可以在不同的用户状态下选择不同的放置策略。但由于放置策略十分离散，它的解空间相当大，那么我们需要对离散空间 dense 化。

        + 我们的 dense 化不是通过模型去做的，而是通过之前使用的重预估监督模型来实现。通过监督模型，我们就可以知道这个 action 下每个位置放置的内容可以带来多少的用户体验和商业价值。

        + 之后，我们可以使用一个神经网络对不同的 action 进行打分。

        + 我们的优化目标是每一步选择能够达到最终的总和价值最大，reward 是长期价值和近期价值的组合。

![image.png](/assets/image_1717232338927_0.png)

      + 两段式训练范式

        + 首先，使用 online policy 的方式，先将模型部署上线生成 online policy 下的数据，作为 off policy 的训练数据放入回放池。

        + 之后，使用 off policy 来训练 Dqn 模型。

  + ### 端上重排

    + 需要解决问题

      + 实时感知

      + 实时反馈

      + 千人千端

      + 算力分配

    + #### 端上重排架构

![image.png](/assets/image_1717232743069_0.png)

    + #### 千人千端

![image.png](/assets/image_1717232855426_0.png)

![image.png](/assets/image_1717232895417_0.png)

      + 

## Ref

  + [读《重排序在快手短视频推荐系统中的演进》有感 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/466241740) 评论区 qujt08 感觉是快手员工？
