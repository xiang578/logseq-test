---
title: GBDT/Question
public: true
deck: being/GBDT
tags:
date: 2024-10-05
lastMod: 2025-03-26
toc: "true"
---


使用 Boosting 解决决策树算法 {{c1 容易过拟合}} 的缺点
为什么使用决策树？ #card
决策树是基于 if-then 规则的集合，易于理解，可解释性强，预测速度快。
需要更少的特征工程，可以不用做特征标准化，可以很好处理字段缺失的数据，自动组合多个特征。
树模型在Gradient boosting这种集成方式下有哪些独特的优势？
样本权重 :-> 方便地将样本权重整合到训练过程中，不需要过采样的方法来调整样本权重。
另外一种方式是 bagging，比如随机森林？
[[Adaboost]]
通过 {{c1 树的层数}} 来平衡 {{c2 表达能力和泛化能力}}
学习器角度 :-> 数据样本波动(采样)对决策树影响大，不同样本集合生成的决策树基分类器随机性比较大。不稳定学习器更适合作为基分类器。
特征工程 :-> 使用不同类型的数据时，不需要做特征标准化/归一化，对数据缺失不敏感
可解释性 :-> 模型可解释性好，学习模型可以输出特征的相对重要程度
特征交叉 :-> 能够自动做多组特征间的特征交叉，具有很好的非线性
为什么 GB 中使用的弱分类器不能是线性分类器？ #card
线性模型无法拟合的残差，无法使用另外一个线性模型去拟合。
如何有多个线性模型，那么也可以将这些模型的系数相加，形成一个新的线性模型。
为什么高维稀疏特征时，lr 的效果会比 gbdt 好?
训练高维稀疏特征时，模型容易 :-> 依赖某个特征。
lr 如何解决 :-> 通过正则项对权重比较大的特征进行惩罚，减轻过拟合。
为什么 GBDT 不容易解决？:-> GBDT 惩罚项主要是树的深度和叶子节点的数量，也许 GBDT 只需要一个节点就能拟合，惩罚项比较小。
GBDT 中的梯度是什么对什么的梯度？[[石塔西]] #card
Loss(y_i, F(x_i)) 对 F(x_i) 求导
`m*n`数据集，如果用GBDT，那么梯度是几维？m维？n维？`m*n`维？或者是与树的深度有关？或者与树的叶子节点的个数有关 [[石塔西]] #card
每一轮对 m 个样本求梯度，所以梯度是 m 维
GBDT/XGBoost 解决过拟合的思路？
正则化角度  #card
在目标函数中添加正则化。
叶子节点个数 + 叶子节点权重的 L2 正则化
CART 正则化剪枝
控制树的深度和最小样本的数量
采样角度 #card
列采样，训练只使用部分特征
子采样，每轮计算使用部分样本
early stopping
shrinkage :-> 调小学习率减少每棵树的影响，增加树的数量，为后面的训练留出更多的空间
GBDT与[[随机森林]]模型相比有什么优缺点？
相同点：最终的结果 {{c1 由多棵树一起}} 决定
RF 决定方式 :-> 多数表决(取平均)
GBDT 决定方式 :-> 加权融合
不同点
[[集成学习]] :-> GBDT 属于 boosting，随机森林属于 bagging
方差与偏差角度 :-> GBDT 不断降低模型的偏差，RF 不断降低模型的方差
训练样本 :-> RF 每次迭代有放回采样，GBDT 使用全部样本
并行性 :-> RF 可以并行生成，GBDT 只能顺序生成
数据敏感性 :-> RF 对异常值不敏感，GBDT 对异常值比较敏感
泛化能力 :-> RF 不易过拟合，GBDT 容易过拟合
